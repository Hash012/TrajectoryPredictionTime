# å…¨é¢è¶…å‚æ•°ä¼˜åŒ–å™¨
import torch
import torch.nn as nn
import torch.optim as optim
from dataload import get_dataloader
from model import VAE
import numpy as np
import logging
import json
import time
from typing import Dict, List, Tuple, Any
import itertools
from sklearn.model_selection import ParameterGrid

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class HyperparameterOptimizer:
    def __init__(self, data_dir: str = './datas'):
        self.data_dir = data_dir
        self.best_config = None
        self.all_results = []
        
    def vae_loss_fn(self, recon_x: torch.Tensor, x: torch.Tensor, mu: torch.Tensor, 
                   logvar: torch.Tensor, kl_weight: float = 0.5, phase_weight: float = 0.1) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """å¯é…ç½®çš„VAEæŸå¤±å‡½æ•°"""
        # é‡æ„æŸå¤±
        magnitude_loss = torch.mean(torch.abs(torch.abs(recon_x) - torch.abs(x)))
        phase_diff = torch.angle(recon_x) - torch.angle(x)
        phase_diff = torch.atan2(torch.sin(phase_diff), torch.cos(phase_diff))
        phase_loss = torch.mean(torch.abs(phase_diff))
        recon_loss = magnitude_loss + phase_weight * phase_loss
        
        # KLæŸå¤±
        kl_loss_real = -0.5 * torch.mean(1 + logvar.real - mu.real.pow(2) - logvar.real.exp())
        kl_loss_imag = -0.5 * torch.mean(1 + logvar.imag - mu.imag.pow(2) - logvar.imag.exp())
        kl_loss = torch.sqrt(kl_loss_real**2 + kl_loss_imag**2)
        
        total_loss = recon_loss + kl_weight * kl_loss
        return total_loss, recon_loss, kl_loss
    
    def train_vae_with_config(self, config: Dict[str, Any], train_loader, test_loader, 
                            quick_mode: bool = True) -> Dict[str, float]:
        """ä½¿ç”¨æŒ‡å®šé…ç½®è®­ç»ƒVAE"""
        # è·å–è¾“å…¥ç»´åº¦
        sample_X, _ = next(iter(train_loader))
        x_dim = sample_X.shape[1]
        z_dim = config['z_dim']
        
        # åˆ›å»ºæ¨¡å‹
        vae = VAE(x_dim, z_dim)
        vae = vae.to(device)
        
        # ä¼˜åŒ–å™¨
        if config['optimizer'] == 'adam':
            optimizer = optim.Adam(vae.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])
        elif config['optimizer'] == 'adamw':
            optimizer = optim.AdamW(vae.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])
        else:
            optimizer = optim.SGD(vae.parameters(), lr=config['lr'], momentum=0.9, weight_decay=config['weight_decay'])
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if config['scheduler'] == 'step':
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.8)
        elif config['scheduler'] == 'cosine':
            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)
        else:
            scheduler = None
        
        # è®­ç»ƒ
        epochs = 20 if quick_mode else 100
        vae.train()
        
        for epoch in range(epochs):
            for x, _ in train_loader:
                if isinstance(x, np.ndarray):
                    x_complex = torch.from_numpy(x).to(dtype=torch.complex64, device=device)
                else:
                    x_complex = x.to(dtype=torch.complex64, device=device)
                
                recon_x, mu, logvar, _ = vae(x_complex)
                loss, _, _ = self.vae_loss_fn(recon_x, x_complex, mu, logvar, 
                                            config['kl_weight'], config['phase_weight'])
                
                optimizer.zero_grad()
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                if config['grad_clip'] > 0:
                    torch.nn.utils.clip_grad_norm_(vae.parameters(), config['grad_clip'])
                
                optimizer.step()
            
            if scheduler:
                scheduler.step()
        
        # è¯„ä¼°
        vae.eval()
        total_recon_loss = 0.0
        total_kl_loss = 0.0
        total_samples = 0
        
        with torch.no_grad():
            for x, _ in test_loader:
                if isinstance(x, np.ndarray):
                    x_complex = torch.from_numpy(x).to(dtype=torch.complex64, device=device)
                else:
                    x_complex = x.to(dtype=torch.complex64, device=device)
                
                recon_x, mu, logvar, _ = vae(x_complex)
                _, recon_loss, kl_loss = self.vae_loss_fn(recon_x, x_complex, mu, logvar,
                                                        config['kl_weight'], config['phase_weight'])
                
                batch_size = x.size(0)
                total_recon_loss += recon_loss.item() * batch_size
                total_kl_loss += kl_loss.item() * batch_size
                total_samples += batch_size
        
        return {
            'recon_loss': total_recon_loss / total_samples,
            'kl_loss': total_kl_loss / total_samples,
            'total_loss': (total_recon_loss + config['kl_weight'] * total_kl_loss) / total_samples
        }
    
    def calculate_score(self, metrics: Dict[str, float], config: Dict[str, Any]) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        recon_score = 1.0 / (1.0 + metrics['recon_loss'])
        kl_score = min(1.0, metrics['kl_loss'] / 0.1)
        
        # å‹ç¼©æ¯”è¯„åˆ†
        compression_ratio = 1198 / config['z_dim']  # å‡è®¾è¾“å…¥ç»´åº¦ä¸º1198
        compression_score = 1.0 / (1.0 + abs(compression_ratio - 8))
        
        # è®­ç»ƒæ•ˆç‡è¯„åˆ†ï¼ˆå­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ç­‰ï¼‰
        efficiency_score = 1.0 / (1.0 + abs(config['lr'] - 1e-4) / 1e-4)
        
        # ç»¼åˆè¯„åˆ†
        total_score = (0.4 * recon_score + 0.3 * kl_score + 
                      0.2 * compression_score + 0.1 * efficiency_score)
        
        return total_score
    
    def optimize_hyperparameters(self, quick_mode: bool = True) -> Dict[str, Any]:
        """ä¼˜åŒ–è¶…å‚æ•°"""
        logger.info("å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–...")
        
        # åŠ è½½æ•°æ®
        train_loader, test_loader, _, _, _ = get_dataloader(self.data_dir, batch_size=16)
        
        # å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
        param_grid = {
            'z_dim': [100, 150, 200, 250],  # æ½œåœ¨ç©ºé—´ç»´åº¦
            'lr': [1e-5, 5e-5, 1e-4, 5e-4],  # å­¦ä¹ ç‡
            'batch_size': [8, 16, 32],  # æ‰¹æ¬¡å¤§å°
            'optimizer': ['adam', 'adamw'],  # ä¼˜åŒ–å™¨
            'weight_decay': [1e-6, 1e-5, 1e-4],  # æƒé‡è¡°å‡
            'kl_weight': [0.3, 0.5, 0.7, 1.0],  # KLæŸå¤±æƒé‡
            'phase_weight': [0.05, 0.1, 0.2],  # ç›¸ä½æŸå¤±æƒé‡
            'scheduler': ['none', 'step', 'cosine'],  # å­¦ä¹ ç‡è°ƒåº¦å™¨
            'grad_clip': [0, 1.0, 5.0]  # æ¢¯åº¦è£å‰ª
        }
        
        # ç”Ÿæˆå‚æ•°ç»„åˆ
        param_combinations = list(ParameterGrid(param_grid))
        logger.info(f"æ€»å…±éœ€è¦æµ‹è¯• {len(param_combinations)} ç§å‚æ•°ç»„åˆ")
        
        # é™åˆ¶æµ‹è¯•æ•°é‡ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
        if quick_mode and len(param_combinations) > 50:
            # éšæœºé€‰æ‹©50ä¸ªç»„åˆ
            np.random.shuffle(param_combinations)
            param_combinations = param_combinations[:50]
            logger.info(f"å¿«é€Ÿæ¨¡å¼ï¼šéšæœºé€‰æ‹© {len(param_combinations)} ç§ç»„åˆè¿›è¡Œæµ‹è¯•")
        
        best_score = -1
        best_config = None
        
        for i, config in enumerate(param_combinations):
            logger.info(f"æµ‹è¯•é…ç½® {i+1}/{len(param_combinations)}: {config}")
            
            try:
                # æ ¹æ®æ‰¹æ¬¡å¤§å°é‡æ–°åŠ è½½æ•°æ®
                if config['batch_size'] != 16:
                    train_loader, test_loader, _, _, _ = get_dataloader(
                        self.data_dir, batch_size=config['batch_size']
                    )
                
                # è®­ç»ƒå’Œè¯„ä¼°
                metrics = self.train_vae_with_config(config, train_loader, test_loader, quick_mode)
                score = self.calculate_score(metrics, config)
                
                result = {
                    'config': config,
                    'metrics': metrics,
                    'score': score
                }
                self.all_results.append(result)
                
                logger.info(f"  é‡æ„æŸå¤±: {metrics['recon_loss']:.4f}")
                logger.info(f"  KLæŸå¤±: {metrics['kl_loss']:.4f}")
                logger.info(f"  ç»¼åˆè¯„åˆ†: {score:.4f}")
                
                if score > best_score:
                    best_score = score
                    best_config = result
                    logger.info(f"  *** æ–°çš„æœ€ä½³é…ç½®ï¼ ***")
                
            except Exception as e:
                logger.error(f"é…ç½® {config} æµ‹è¯•å¤±è´¥: {str(e)}")
                continue
        
        self.best_config = best_config
        return best_config
    
    def analyze_results(self) -> Dict[str, Any]:
        """åˆ†æä¼˜åŒ–ç»“æœ"""
        if not self.all_results:
            return {}
        
        # æŒ‰è¯„åˆ†æ’åº
        self.all_results.sort(key=lambda x: x['score'], reverse=True)
        
        # åˆ†ææœ€ä½³é…ç½®
        best = self.all_results[0]
        
        # åˆ†æå„å‚æ•°çš„å½±å“
        param_analysis = {}
        for param in ['z_dim', 'lr', 'kl_weight', 'phase_weight']:
            values = [r['config'][param] for r in self.all_results]
            scores = [r['score'] for r in self.all_results]
            
            # è®¡ç®—å‚æ•°å€¼ä¸è¯„åˆ†çš„ç›¸å…³æ€§
            unique_values = sorted(set(values))
            avg_scores = []
            for val in unique_values:
                matching_scores = [s for v, s in zip(values, scores) if v == val]
                avg_scores.append(np.mean(matching_scores))
            
            param_analysis[param] = {
                'values': unique_values,
                'avg_scores': avg_scores,
                'best_value': unique_values[np.argmax(avg_scores)]
            }
        
        analysis = {
            'best_config': best,
            'top_5_configs': self.all_results[:5],
            'param_analysis': param_analysis,
            'total_tests': len(self.all_results)
        }
        
        return analysis
    
    def save_results(self, filename: str = 'hyperparameter_optimization_results.json'):
        """ä¿å­˜ä¼˜åŒ–ç»“æœ"""
        analysis = self.analyze_results()
        
        # å‡†å¤‡ä¿å­˜çš„æ•°æ®
        save_data = {
            'best_config': analysis['best_config'],
            'top_5_configs': analysis['top_5_configs'],
            'param_analysis': analysis['param_analysis'],
            'total_tests': analysis['total_tests'],
            'all_results': self.all_results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ° {filename}")
        
        # æ‰“å°æœ€ä½³é…ç½®
        best = analysis['best_config']
        logger.info("\n" + "="*50)
        logger.info("ğŸ¯ æœ€ä½³è¶…å‚æ•°é…ç½®:")
        logger.info("="*50)
        for param, value in best['config'].items():
            logger.info(f"{param:15}: {value}")
        logger.info(f"{'é‡æ„æŸå¤±':15}: {best['metrics']['recon_loss']:.4f}")
        logger.info(f"{'KLæŸå¤±':15}: {best['metrics']['kl_loss']:.4f}")
        logger.info(f"{'ç»¼åˆè¯„åˆ†':15}: {best['score']:.4f}")
        logger.info("="*50)

def main():
    """ä¸»å‡½æ•°"""
    optimizer = HyperparameterOptimizer()
    
    # å¿«é€Ÿä¼˜åŒ–æ¨¡å¼
    logger.info("å¼€å§‹å¿«é€Ÿè¶…å‚æ•°ä¼˜åŒ–...")
    best_config = optimizer.optimize_hyperparameters(quick_mode=True)
    
    if best_config:
        optimizer.save_results()
        
        # ä½¿ç”¨æœ€ä½³é…ç½®è¿›è¡Œå®Œæ•´è®­ç»ƒ
        logger.info("\nä½¿ç”¨æœ€ä½³é…ç½®è¿›è¡Œå®Œæ•´è®­ç»ƒ...")
        full_config = optimizer.optimize_hyperparameters(quick_mode=False)
        optimizer.save_results('full_hyperparameter_optimization_results.json')
    else:
        logger.error("è¶…å‚æ•°ä¼˜åŒ–å¤±è´¥")

if __name__ == '__main__':
    main() 